{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00031388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "from acell import preprocess_data,load_assist_data\n",
    "from tgcn import tgcnCell\n",
    "from gru import GRUCell \n",
    "\n",
    "from visualization import plot_result,plot_error\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "###### Settings ######\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('training_epoch', 2500, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('gru_units', 100, 'hidden units of gru.')\n",
    "flags.DEFINE_integer('seq_len', 5, 'time length of inputs.')\n",
    "flags.DEFINE_integer('pre_len', 1, 'time length of prediction.')\n",
    "flags.DEFINE_float('train_rate', 0.8, 'rate of training set.')\n",
    "flags.DEFINE_integer('batch_size', 64, 'batch size.')\n",
    "flags.DEFINE_string('dataset', '3611817550', 'dataset')\n",
    "flags.DEFINE_string('model_name', 'tgcn', 'ast-gcn/tgcn/gru')\n",
    "flags.DEFINE_integer('scheme', 14, 'scheme')\n",
    "                            #记得改dim\n",
    "flags.DEFINE_string('noise_name', 'None', 'None or Gauss or Possion')\n",
    "flags.DEFINE_integer('noise_param', 0, 'Parameter for noise')\n",
    "#flags.DEFINE_integer('dim', 20, 'DIM')\n",
    "\n",
    "model_name = FLAGS.model_name\n",
    "noise_name = FLAGS.noise_name\n",
    "data_name = FLAGS.dataset\n",
    "train_rate =  FLAGS.train_rate\n",
    "seq_len = FLAGS.seq_len\n",
    "output_dim = pre_len = FLAGS.pre_len\n",
    "batch_size = FLAGS.batch_size\n",
    "lr = FLAGS.learning_rate\n",
    "training_epoch = FLAGS.training_epoch\n",
    "gru_units = FLAGS.gru_units\n",
    "#dim = FLAGS.dim\n",
    "scheme = FLAGS.scheme\n",
    "PG = FLAGS.noise_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751cbaf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### load data ######\n",
    "if data_name == '3611817550':\n",
    "    data, adj = load_assist_data('3611817550', model_name)\n",
    "#if data_name == 'sh':\n",
    "#    data, adj = load_sh_data('sh')\n",
    "\n",
    "### Perturbation Analysis\n",
    "def MaxMinNormalization(x,Max,Min):\n",
    "    x = (x-Min)/(Max-Min)\n",
    "    return x\n",
    "\n",
    "if noise_name == 'Gauss':\n",
    "    Gauss = np.random.normal(0,PG,size=data.shape)\n",
    "    noise_Gauss = MaxMinNormalization(Gauss,np.max(Gauss),np.min(Gauss))\n",
    "    data = data + noise_Gauss\n",
    "elif noise_name == 'Possion':\n",
    "    Possion = np.random.poisson(PG,size=data.shape)\n",
    "    noise_Possion = MaxMinNormalization(Possion,np.max(Possion),np.min(Possion))\n",
    "    data = data + noise_Possion\n",
    "else:\n",
    "    data = data\n",
    "\n",
    "time_len = data.shape[0]\n",
    "num_nodes = data.shape[1]\n",
    "data1 =np.mat(data,dtype=np.float32)\n",
    "\n",
    "\n",
    "############## normalization ###############\n",
    "price_frame = data1\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "data1 = min_max_scaler.fit_transform(price_frame)\n",
    "#max_value = np.max(data1)\n",
    "max_value = 1\n",
    "#data1  = data1/max_value\n",
    "\n",
    "#data1.columns = data.columns\n",
    "if model_name == 'ast-gcn':\n",
    "    if scheme == 1:\n",
    "        name = 'add direction dim'\n",
    "    elif scheme == 2:\n",
    "        name = 'add beach_width dim'\n",
    "    elif scheme == 3:\n",
    "        name = 'add Reservoir_water_level dim'\n",
    "    elif scheme == 4:\n",
    "        name = 'add phreatic_line dim'\n",
    "    elif scheme == 5:\n",
    "        name = 'add precipitation dim'\n",
    "    elif scheme == 6:\n",
    "        name = 'add dynamic attribute dim'\n",
    "    else:\n",
    "        name = 'add direction + dynamic attributes dim'\n",
    "\n",
    "if model_name == 'tgcn':\n",
    "    name = 'tgcn'\n",
    "if model_name == 'gru':\n",
    "    name = 'gru'\n",
    "\n",
    "print('model:', model_name)\n",
    "print('scheme:', name)\n",
    "print('noise_name:', noise_name)\n",
    "print('noise_param:', PG)\n",
    "\n",
    "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len, model_name, scheme)\n",
    "\n",
    "totalbatch = int(trainX.shape[0]/batch_size)\n",
    "training_data_count = len(trainX)\n",
    "\n",
    "def TGCN(_X, _weights, _biases):\n",
    "    ###\n",
    "    cell_1 = tgcnCell(gru_units, adj, num_nodes=num_nodes)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n",
    "    _X = tf.unstack(_X, axis=1)\n",
    "    outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32)\n",
    "    m = []\n",
    "    for i in outputs:\n",
    "        o = tf.reshape(i,shape=[-1,num_nodes,gru_units])\n",
    "        o = tf.reshape(o,shape=[-1,gru_units])\n",
    "        m.append(o)\n",
    "    last_output = m[-1]\n",
    "    output = tf.matmul(last_output, _weights['out']) + _biases['out']\n",
    "    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n",
    "    output = tf.transpose(output, perm=[0,2,1])\n",
    "    output = tf.reshape(output, shape=[-1,num_nodes])\n",
    "    return output, m, states\n",
    "\n",
    "def GRU(_X, weights, biases):\n",
    "    ###\n",
    "    cell_1 = GRUCell(gru_units,num_nodes=num_nodes)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n",
    "    _X = tf.unstack(_X, axis=1)\n",
    "    outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32)\n",
    "    m = []\n",
    "    for i in outputs:\n",
    "        o = tf.reshape(i,shape=[-1,num_nodes,gru_units])\n",
    "        o = tf.reshape(o,shape=[-1,gru_units])\n",
    "        m.append(o)\n",
    "    last_output = m[-1]\n",
    "    output = tf.matmul(last_output, weights['out']) + biases['out']\n",
    "    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n",
    "    output = tf.transpose(output, perm=[0,2,1])\n",
    "    output = tf.reshape(output, shape=[-1,num_nodes])\n",
    "\n",
    "    return output, m, states\n",
    "\n",
    "###### placeholders ######\n",
    "if model_name == 'ast-gcn':\n",
    "    if scheme == 1:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len+1, num_nodes])\n",
    "    elif scheme == 2:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len*2+pre_len, num_nodes])\n",
    "    elif scheme == 3:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len*2+pre_len, num_nodes])\n",
    "    elif scheme == 4:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len*2+pre_len, num_nodes])\n",
    "    elif scheme == 5:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len*2+pre_len, num_nodes])\n",
    "    elif scheme == 6:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len+pre_len, num_nodes])\n",
    "    else:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len*5+pre_len*4+1, num_nodes])#公式为 seq_len *（特征矩阵个数+动态因子个数）+ pre_len *（动态因子个数）+ 1\n",
    "\n",
    "else:\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\n",
    "\n",
    "labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([gru_units, pre_len], mean=1.0), name='weight_o')}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([pre_len]),name='bias_o')}\n",
    "\n",
    "if model_name == 'gru':\n",
    "    pred,ttts,ttto = GRU(inputs, weights, biases)\n",
    "if model_name == 'tgcn':\n",
    "    pred,ttts,ttto = TGCN(inputs, weights, biases)\n",
    "\n",
    "y_pred = pred\n",
    "\n",
    "###### optimizer ######\n",
    "lambda_loss = 0.0015\n",
    "Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "label = tf.reshape(labels, [-1,num_nodes])\n",
    "##loss\n",
    "print('y_pred_shape:', y_pred.shape)\n",
    "print('label_shape:', label.shape)\n",
    "loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)\n",
    "#loss = tf.reduce_mean(tf.square(label-y_pred))\n",
    "##rmse\n",
    "error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "###### Initialize session ######\n",
    "variables = tf.global_variables()\n",
    "saver = tf.train.Saver(tf.global_variables())  \n",
    "#sess = tf.Session()\n",
    "\n",
    "#改动态获取GPU\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\n",
    "gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#out = 'out/%s'%(model_name)\n",
    "out = 'out/%s_%s'%(model_name,noise_name)\n",
    "path1 = 'MinMAXTGCNadj2%s_%s_%s_lr%r_batch%r_unit%r_seq%r_pre%r_epoch%r_scheme%r_PG%r'%(model_name,name,data_name,lr,batch_size,gru_units,seq_len,pre_len,training_epoch,scheme,PG)\n",
    "path = os.path.join(out,path1)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "###### evaluation ######\n",
    "def evaluation1(a,b):\n",
    "    rmse = math.sqrt(mean_squared_error(a,b))\n",
    "    mae = mean_absolute_error(a, b)\n",
    "    return rmse, mae\n",
    "def evaluation2(a,b):\n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
    "    var = 1-(np.var(a-b))/np.var(a)\n",
    "    return 1-F_norm, r2, var\n",
    "\n",
    "x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n",
    "test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]\n",
    "test_renor_pred = []\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    for m in range(totalbatch):\n",
    "        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n",
    "        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n",
    "        _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],\n",
    "                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n",
    "        batch_loss.append(loss1)\n",
    "        batch_rmse.append(rmse1 * max_value)\n",
    "\n",
    "     # Test completely at every epoch\n",
    "    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n",
    "                                         feed_dict = {inputs:testX, labels:testY})\n",
    "\n",
    "    testoutput = np.abs(test_output)\n",
    "    test_label = np.reshape(testY,[-1,num_nodes])\n",
    "    \n",
    "# ############## normalization ###############\n",
    "#     price_frame = pd.DataFrame(pd.read_excel(r'./data_35-5_under10float/3611817550_feature_matrix_X.xlsx', header=0))\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "#     x_minmax = min_max_scaler.fit_transform(price_frame)\n",
    "    \n",
    "    acc, r2_score, var_score = evaluation2(test_label, testoutput)\n",
    "    \n",
    "############## renormalization ###############  \n",
    "#     test_label1 = test_label * max_value\n",
    "#     test_output1 = testoutput * max_value\n",
    "    test_label1 = min_max_scaler.inverse_transform(test_label)\n",
    "    test_output1 = min_max_scaler.inverse_transform(testoutput)\n",
    "    \n",
    "    rmse, mae = evaluation1(test_label1, test_output1)\n",
    "    test_loss.append(loss2)\n",
    "#     test_rmse.append(rmse * max_value)\n",
    "#     test_mae.append(mae * max_value)\n",
    "    test_rmse.append(rmse)\n",
    "    test_mae.append(mae)\n",
    "    test_acc.append(acc)\n",
    "    test_r2.append(r2_score)\n",
    "    test_var.append(var_score)\n",
    "    test_pred.append(test_output1)\n",
    "    test_renor_pred.append(testoutput)\n",
    "    \n",
    "    print('Iter:{}'.format(epoch),\n",
    "          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n",
    "          'test_loss:{:.4}'.format(loss2),\n",
    "          'test_rmse:{:.4}'.format(rmse),\n",
    "          'test_acc:{:.4}'.format(acc))\n",
    "    \n",
    "    if (epoch % 500 == 0):        \n",
    "        saver.save(sess, path+'/model_100/ASTGCN_pre_%r'%epoch, global_step = epoch)\n",
    "        \n",
    "time_end = time.time()\n",
    "print(time_end-time_start,'s')\n",
    "\n",
    "############## visualization ###############\n",
    "#x = [i for i in range(training_epoch)]\n",
    "b = int(len(batch_rmse)/totalbatch)\n",
    "batch_rmse1 = [i for i in batch_rmse]\n",
    "train_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "batch_loss1 = [i for i in batch_loss]\n",
    "train_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "#test_rmse = [float(i) for i in test_rmse]\n",
    "\n",
    "index = test_rmse.index(np.min(test_rmse))\n",
    "test_result = test_pred[index]\n",
    "test_renor_pred = test_renor_pred[index]\n",
    "var = pd.DataFrame(test_result)\n",
    "var2 = pd.DataFrame(test_label1)\n",
    "var.to_csv(path+'/test_prediction.csv',index = False,header = False)\n",
    "var2.to_csv(path+'/test_true.csv',index = False,header = False)\n",
    "plot_result(test_result,test_label1,path)\n",
    "plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)\n",
    "evalution = []\n",
    "evalution.append(np.min(test_rmse))\n",
    "evalution.append(test_mae[index])\n",
    "evalution.append(test_acc[index])\n",
    "evalution.append(test_r2[index])\n",
    "evalution.append(test_var[index])\n",
    "evalution = pd.DataFrame(evalution)\n",
    "evalution.to_csv(path+'/evalution_all.csv',index=False,header=None)\n",
    "\n",
    "print('model_name:', model_name)\n",
    "print('scheme:', scheme)\n",
    "print('name:', name)\n",
    "print('noise_name:', noise_name)\n",
    "print('PG:', PG)\n",
    "print('min_rmse:%r'%(np.min(test_rmse)),\n",
    "      'min_mae:%r'%(test_mae[index]),\n",
    "      'max_acc:%r'%(test_acc[index]),\n",
    "      'r2:%r'%(test_r2[index]),\n",
    "      'var:%r'%test_var[index])\n",
    "\n",
    "test_rmse.clear()\n",
    "test_mae.clear()\n",
    "test_acc.clear()\n",
    "test_r2.clear()\n",
    "test_var.clear()\n",
    "j=1\n",
    "for i in range(num_nodes):\n",
    "    a = test_label1[:,i]\n",
    "    b = test_result[:,i]\n",
    "    a = np.array(a)\n",
    "    a = np.reshape(a,[-1, pre_len])\n",
    "    b = np.array(np.transpose(np.mat(b)))\n",
    "    b = b.repeat(pre_len ,axis=1)\n",
    "    rmse, mae = evaluation1(a, b)\n",
    "    \n",
    "    a = test_label[:,i]\n",
    "    b = test_renor_pred[:,i]\n",
    "    a = np.array(a)\n",
    "    a = np.reshape(a,[-1, pre_len])\n",
    "    b = np.array(np.transpose(np.mat(b)))\n",
    "    b = b.repeat(pre_len ,axis=1)\n",
    "    acc,r2,var = evaluation2(a, b)\n",
    "    if r2<0:\n",
    "        r2=0\n",
    "    print('第%r列 TGCN_rmse:%r\\n'%(j,rmse),\n",
    "          '第%r列 TGCN_mae:%r\\n'%(j,mae),\n",
    "          '第%r列 TGCN_acc:%r\\n'%(j,acc),\n",
    "          '第%r列 TGCN_r2:%r\\n'%(j,r2),\n",
    "          '第%r列 TGCN_var:%r\\n'%(j,var))\n",
    "    print('\\n')\n",
    "    test_rmse.append(rmse)\n",
    "    test_mae.append(mae)\n",
    "    test_acc.append(acc)\n",
    "    test_r2.append(r2)\n",
    "    test_var.append(var)\n",
    "    j += 1\n",
    "rmse = np.array(test_rmse)\n",
    "mae = np.array(test_mae)\n",
    "acc = np.array(test_acc)\n",
    "r2 = np.array(test_r2)\n",
    "var = np.array(test_var)\n",
    "rmse = np.mean(rmse)\n",
    "mae = np.mean(mae)\n",
    "acc = np.mean(acc)\n",
    "r2 = np.mean(r2)\n",
    "var = np.mean(var)\n",
    "print('mean_rmse:%r'%(rmse),\n",
    "      'mean_mae:%r'%(mae),\n",
    "      'meam_acc:%r'%(acc),\n",
    "      'mean_r2:%r'%(r2),\n",
    "      'mean_var:%r'%var)\n",
    "evalution2 = []\n",
    "evalution2.append(rmse)\n",
    "evalution2.append(mae)\n",
    "evalution2.append(acc)\n",
    "evalution2.append(r2)\n",
    "evalution2.append(var)\n",
    "evalution2 = pd.DataFrame(evalution2)\n",
    "evalution2.to_csv(path+'/evalution_mean.csv',index=False,header=None)\n",
    "\n",
    "\n",
    "#记得回溯minmax归一化结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f841d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577abb27",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
