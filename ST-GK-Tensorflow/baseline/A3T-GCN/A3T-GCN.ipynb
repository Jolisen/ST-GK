{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa8ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "s Tensor(\"mul:0\", shape=(?, 5), dtype=float32)\n",
      "bata Tensor(\"Softmax:0\", shape=(?, 5), dtype=float32)\n",
      "context Tensor(\"transpose_1:0\", shape=(?, 30, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "from input_data import preprocess_data,load_sz_data,load_los_data\n",
    "from tgcn import tgcnCell\n",
    "from gru import GRUCell \n",
    "\n",
    "from visualization import plot_result,plot_error\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "###### Settings ######\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('training_epoch', 2500, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('gru_units', 100, 'hidden units of gru.')\n",
    "flags.DEFINE_integer('seq_len', 5, 'time length of inputs.')\n",
    "flags.DEFINE_integer('pre_len', 1, 'time length of prediction.')\n",
    "flags.DEFINE_float('train_rate', 0.8, 'rate of training set.')\n",
    "flags.DEFINE_integer('batch_size', 64, 'batch size.')\n",
    "flags.DEFINE_string('dataset', '3611817550', 'sz or los.')\n",
    "flags.DEFINE_string('model_name', 'TGCN_att', 'TGCN_att')\n",
    "model_name = FLAGS.model_name\n",
    "data_name = FLAGS.dataset\n",
    "train_rate =  FLAGS.train_rate\n",
    "seq_len = FLAGS.seq_len\n",
    "output_dim = pre_len = FLAGS.pre_len\n",
    "batch_size = FLAGS.batch_size\n",
    "lr = FLAGS.learning_rate\n",
    "training_epoch = FLAGS.training_epoch\n",
    "gru_units = FLAGS.gru_units\n",
    "\n",
    "###### load data ######\n",
    "if data_name == '3611817550':\n",
    "    data, adj = load_sz_data('3611817550')\n",
    "if data_name == 'los':\n",
    "    data, adj = load_los_data('los')\n",
    "\n",
    "time_len = data.shape[0]\n",
    "num_nodes = data.shape[1]\n",
    "data1 =np.mat(data,dtype=np.float32)\n",
    "### Perturbation Analysis\n",
    "#noise = np.random.normal(0,0.2,size=data.shape)\n",
    "#noise = np.random.poisson(16,size=data.shape)\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler.fit(noise)\n",
    "#noise = scaler.transform(noise)\n",
    "#data1 = data1 + noise\n",
    "#### normalization\n",
    "price_frame = data1\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "data1 = min_max_scaler.fit_transform(price_frame)\n",
    "#max_value = np.max(data1)\n",
    "max_value = 1\n",
    "#data1  = data1/max_value\n",
    "\n",
    "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len)\n",
    "\n",
    "totalbatch = int(trainX.shape[0]/batch_size)\n",
    "training_data_count = len(trainX)\n",
    "\n",
    "def TGCN(_X, weights, biases):\n",
    "    ###\n",
    "    cell_1 = tgcnCell(gru_units, adj, num_nodes=num_nodes)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n",
    "    _X = tf.unstack(_X, axis=1)\n",
    "    outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32)\n",
    "\n",
    "    out = tf.concat(outputs, axis=0)\n",
    "    out = tf.reshape(out, shape=[seq_len,-1,num_nodes,gru_units])\n",
    "    out = tf.transpose(out, perm=[1,0,2,3])\n",
    "\n",
    "    last_output,alpha = self_attention1(out, weight_att, bias_att)\n",
    "\n",
    "    output = tf.reshape(last_output,shape=[-1,seq_len])\n",
    "    output = tf.matmul(output, weights['out']) + biases['out']\n",
    "    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n",
    "    output = tf.transpose(output, perm=[0,2,1])\n",
    "    output = tf.reshape(output, shape=[-1,num_nodes])\n",
    "\n",
    "    return output, outputs, states, alpha\n",
    "    \n",
    "def self_attention1(x, weight_att,bias_att):\n",
    "    x = tf.matmul(tf.reshape(x,[-1,gru_units]),weight_att['w1']) + bias_att['b1']\n",
    "#    f = tf.layers.conv2d(x, ch // 8, kernel_size=1, kernel_initializer=tf.variance_scaling_initializer())\n",
    "    f = tf.matmul(tf.reshape(x, [-1, num_nodes]), weight_att['w2']) + bias_att['b2']\n",
    "    g = tf.matmul(tf.reshape(x, [-1, num_nodes]), weight_att['w2']) + bias_att['b2']\n",
    "    h = tf.matmul(tf.reshape(x, [-1, num_nodes]), weight_att['w2']) + bias_att['b2']\n",
    "\n",
    "    f1 = tf.reshape(f, [-1,seq_len])\n",
    "    g1 = tf.reshape(g, [-1,seq_len])\n",
    "    h1 = tf.reshape(h, [-1,seq_len])\n",
    "    s = g1 * f1\n",
    "    print('s',s)\n",
    "\n",
    "    beta = tf.nn.softmax(s, dim=-1)  # attention map\n",
    "    print('bata',beta)\n",
    "    context = tf.expand_dims(beta,2) * tf.reshape(x,[-1,seq_len,num_nodes])\n",
    "\n",
    "    context = tf.transpose(context,perm=[0,2,1])\n",
    "    print('context', context)\n",
    "    return context, beta \n",
    "def self_attention(x, ch, weight_att, bias_att):\n",
    "    f = tf.matmul(tf.reshape(x, [-1, gru_units]), weight_att['w'])\n",
    "    g = tf.matmul(tf.reshape(x, [-1, gru_units]), weight_att['w']) + bias_att['b_att']\n",
    "    h = tf.matmul(tf.reshape(x, [-1, gru_units]), weight_att['w']) + bias_att['b_att']\n",
    "    print('h',h)\n",
    "\n",
    "    f = tf.reshape(f, [-1,num_nodes])\n",
    "    g = tf.reshape(g, [-1,num_nodes])\n",
    "    h = tf.reshape(h, [-1,num_nodes])    \n",
    "    s = g * f\n",
    "    print('s',s)\n",
    "\n",
    "    beta = tf.nn.softmax(s, dim=-1)  # attention map\n",
    "    print('bata',beta)\n",
    "#    o = tf.matmul(beta, h) # [bs, N, C]\n",
    "    o = beta * h\n",
    "    print('o',o)\n",
    "    gamma = tf.get_variable(\"gamma\", [1], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#    o = tf.reshape(o, shape=x.shape) # [bs, h, w, C]\n",
    "#    o = tf.reshape(o, shape=x.shape)\n",
    "    o = tf.expand_dims(o, 2)\n",
    "    x = gamma * o + x\n",
    "    print('x',x)\n",
    "#    x = tf.reduce_sum(x, 2)\n",
    "    return x, beta    \n",
    "\n",
    "###### placeholders ######\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
    "\n",
    "# weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([seq_len, pre_len], mean=1.0), name='weight_o')}\n",
    "bias = {\n",
    "    'out': tf.Variable(tf.random_normal([pre_len]),name='bias_o')}\n",
    "weight_att={\n",
    "    'w1':tf.Variable(tf.random_normal([gru_units,1], stddev=0.1),name='att_w1'),\n",
    "    'w2':tf.Variable(tf.random_normal([num_nodes,1], stddev=0.1),name='att_w2')}\n",
    "bias_att = {\n",
    "    'b1': tf.Variable(tf.random_normal([1]),name='att_b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([1]),name='att_b2')}\n",
    "\n",
    "if model_name == 'TGCN_att':\n",
    "    pred,ttto,ttts,alpha = TGCN(inputs, weights, bias)\n",
    "#    print(alpha)\n",
    "if model_name == 'GRU':\n",
    "    pred,ttts,ttto = GRU(inputs, weights, bias)    \n",
    "if model_name == 'GCN':\n",
    "    model = GCN(gru_units, inputs, output_dim)\n",
    "    \n",
    "y_pred = pred\n",
    "#print('ooooo',y_pred)\n",
    "             \n",
    "\n",
    "###### optimizer ######\n",
    "lambda_loss = 0.0015\n",
    "Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "label = tf.reshape(labels, [-1,num_nodes])\n",
    "##loss\n",
    "loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)\n",
    "##rmse\n",
    "error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "###### Initialize session ######\n",
    "variables = tf.global_variables()\n",
    "saver = tf.train.Saver(tf.global_variables())  \n",
    "#sess = tf.Session()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#out = 'out/%s'%(model_name)\n",
    "out = 'out/%s'%(model_name)\n",
    "path1 = '%s_%s_lr%r_batch%r_unit%r_seq%r_pre%r_epoch%r'%(model_name,data_name,lr,batch_size,gru_units,seq_len,pre_len,training_epoch)\n",
    "path = os.path.join(out,path1)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### evaluation ######\n",
    "def evaluation1(a,b):\n",
    "    rmse = math.sqrt(mean_squared_error(a,b))\n",
    "    mae = mean_absolute_error(a, b)\n",
    "    return rmse, mae\n",
    "def evaluation2(a,b):\n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
    "    var = 1-(np.var(a-b))/np.var(a)\n",
    "    return 1-F_norm, r2, var\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "    return batch_s\n",
    "    \n",
    "\n",
    "x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n",
    "test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]\n",
    "test_no_renor_pred = []\n",
    "for epoch in range(training_epoch):\n",
    "    for m in range(totalbatch):\n",
    "        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n",
    "        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n",
    "        _, loss1, rmse1, train_output, alpha1 = sess.run([optimizer, loss, error, y_pred, alpha],\n",
    "                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n",
    "        batch_loss.append(loss1)\n",
    "        batch_rmse.append(rmse1 * max_value)\n",
    "\n",
    "     # Test completely at every epoch\n",
    "    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n",
    "                                         feed_dict = {inputs:testX, labels:testY})\n",
    "    \n",
    "    testoutput = np.abs(test_output)\n",
    "    test_label = np.reshape(testY,[-1,num_nodes])\n",
    "    \n",
    "# ############## normalization ###############\n",
    "#     price_frame = pd.DataFrame(pd.read_excel(r'./data_35-5_under10float/3611817550_feature_matrix_X.xlsx', header=0))\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "#     x_minmax = min_max_scaler.fit_transform(price_frame)\n",
    "    \n",
    "    acc, r2_score, var_score = evaluation2(test_label, testoutput)\n",
    "    \n",
    "############## renormalization ###############  \n",
    "#     test_label1 = test_label * max_value\n",
    "#     test_output1 = testoutput * max_value\n",
    "    test_label1 = min_max_scaler.inverse_transform(test_label)\n",
    "    test_output1 = min_max_scaler.inverse_transform(testoutput)\n",
    "    \n",
    "    rmse, mae = evaluation1(test_label1, test_output1)\n",
    "    test_loss.append(loss2)\n",
    "#     test_rmse.append(rmse * max_value)\n",
    "#     test_mae.append(mae * max_value)\n",
    "    test_rmse.append(rmse)\n",
    "    test_mae.append(mae)\n",
    "    test_acc.append(acc)\n",
    "    test_r2.append(r2_score)\n",
    "    test_var.append(var_score)\n",
    "    test_pred.append(test_output1)\n",
    "    test_no_renor_pred.append(testoutput)\n",
    "    \n",
    "    \n",
    "    print('Iter:{}'.format(epoch),\n",
    "          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n",
    "          'test_loss:{:.4}'.format(loss2),\n",
    "          'test_rmse:{:.4}'.format(rmse),\n",
    "          'test_acc:{:.4}'.format(acc))\n",
    "    \n",
    "    if (epoch % 500 == 0):        \n",
    "        saver.save(sess, path+'/model_100/graphGRU_pre_%r'%epoch, global_step = epoch)\n",
    "        \n",
    "time_end = time.time()\n",
    "print(time_end-time_start,'s')\n",
    "\n",
    "############## visualization ###############\n",
    "#x = [i for i in range(training_epoch)]\n",
    "b = int(len(batch_rmse)/totalbatch)\n",
    "batch_rmse1 = [i for i in batch_rmse]\n",
    "train_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "batch_loss1 = [i for i in batch_loss]\n",
    "train_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "#test_rmse = [float(i) for i in test_rmse]\n",
    "# var = pd.DataFrame(batch_loss1)\n",
    "# var.to_csv(path+'/batch_loss.csv',index = False,header = False)\n",
    "# var = pd.DataFrame(train_loss)\n",
    "# var.to_csv(path+'/train_loss.csv',index = False,header = False)\n",
    "# var = pd.DataFrame(batch_rmse1)\n",
    "# var.to_csv(path+'/batch_rmse.csv',index = False,header = False)\n",
    "# var = pd.DataFrame(train_rmse)\n",
    "# var.to_csv(path+'/train_rmse.csv',index = False,header = False)\n",
    "# var = pd.DataFrame(test_loss)\n",
    "# var.to_csv(path+'/test_loss.csv',index = False,header = False)\n",
    "# var = pd.DataFrame(test_acc)\n",
    "# var.to_csv(path+'/test_acc.csv',index = False,header = False)\n",
    "# var = pd.DataFrame(test_rmse)\n",
    "# var.to_csv(path+'/test_rmse.csv',index = False,header = False)\n",
    "\n",
    "\n",
    "index = test_rmse.index(np.min(test_rmse))\n",
    "test_result = test_pred[index]\n",
    "test_no_renor_pred = test_no_renor_pred[index]\n",
    "var = pd.DataFrame(test_result)\n",
    "var2 = pd.DataFrame(test_label1)\n",
    "var.to_csv(path+'/test_prediction.csv',index = False,header = False)\n",
    "var2.to_csv(path+'/test_true.csv',index = False,header = False)\n",
    "plot_result(test_result,test_label1,path)\n",
    "plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)\n",
    "\n",
    "fig1 = plt.figure(figsize=(7,3))\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "plt.plot(np.sum(alpha1,0))\n",
    "plt.savefig(path+'/alpha.jpg',dpi=500)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(np.mat(np.sum(alpha1,0)))\n",
    "plt.savefig(path+'/alpha11.jpg',dpi=500)\n",
    "plt.show()\n",
    "\n",
    "evalution = []\n",
    "evalution.append(np.min(test_rmse))\n",
    "evalution.append(test_mae[index])\n",
    "evalution.append(test_acc[index])\n",
    "evalution.append(test_r2[index])\n",
    "evalution.append(test_var[index])\n",
    "evalution = pd.DataFrame(evalution)\n",
    "evalution.to_csv(path+'/evalution_all.csv',index=False,header=None)\n",
    "\n",
    "\n",
    "print('model_name:', model_name)\n",
    "print('min_rmse:%r'%(np.min(test_rmse)),\n",
    "      'min_mae:%r'%(test_mae[index]),\n",
    "      'max_acc:%r'%(test_acc[index]),\n",
    "      'r2:%r'%(test_r2[index]),\n",
    "      'var:%r'%test_var[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7401b5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6299105",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
